<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ECCV 2024 2nd Workshop on Vision-Centric Autonomous Driving (VCAD)</title>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Montserrat', sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
            background-color: #f0f2f5;
            color: #333;
        }
        .container {
            width: 90%;
            max-width: 900px;
            margin: 20px auto;
            padding: 20px;
            background: #fff;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
            border-radius: 8px;
        }
        h1, h2, h3, h4, h5 {
            text-align: center;
            color: #1a237e;
        }
        .intro, .dates, .keynotes, .organizers, .contact {
            margin: 20px 0;
        }
        .intro ul, .dates ul {
            list-style: none;
            padding: 0;
        }
        .intro li, .dates li {
            margin: 10px 0;
            padding-left: 20px;
            position: relative;
        }
        .intro li::before, .dates li::before {
            content: '•';
            position: absolute;
            left: 0;
            color: #1a237e;
            font-weight: bold;
        }
        .speakers, .organizers {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
        }
        .person {
            text-align: center;
            margin: 20px;
            width: calc(100% - 40px);
            max-width: 200px;
        }
        .person img {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            object-fit: cover;
            border: 3px solid #1a237e;
            transition: transform 0.3s ease;
        }
        .person img:hover {
            transform: scale(1.1);
        }
        .person p {
            margin: 10px 0 0 0;
        }
        .person a {
            text-decoration: none;
            color: #333;
            font-weight: bold;
        }
        .person a:hover {
            color: #1a237e;
        }
        .contact a {
            color: #1a237e;
            text-decoration: none;
        }
        .contact a:hover {
            text-decoration: underline;
        }
        footer {
            text-align: center;
            margin: 20px 0;
            font-size: 0.9em;
            color: #777;
        }
        nav {
            background-color: #1a237e;
            padding: 10px 0;
            text-align: center;
        }
        nav a {
            color: #fff;
            margin: 0 15px;
            text-decoration: none;
            font-weight: bold;
        }
        nav a:hover {
            text-decoration: underline;
        }
        .submission-button {
            display: block;
            margin: 20px auto;
            padding: 10px 20px;
            background-color: #1a237e;
            color: #fff;
            text-align: center;
            border-radius: 5px;
            text-decoration: none;
            font-weight: bold;
        }
        .submission-button:hover {
            background-color: #303f9f;
        }
        .schedule {
            margin: 20px 0;
        }
        .schedule h3 {
            color: #1a237e;
            margin-bottom: 10px;
        }
        .schedule p {
            margin-bottom: 10px;
        }
        @media (min-width: 600px) {
            .person {
                flex: 1 1 calc(25% - 40px);
                max-width: calc(25% - 40px);
            }
        }
    </style>
</head>
<body>
    <nav>
        <a href="#intro">Introduction</a>
        <a href="#dates">Key Dates</a>
        <a href="#keynotes">Keynote Speakers</a>
        <a href="#organizers">Organizers</a>
        <a href="#contact">Contact</a>
    </nav>

    <div class="container">
        <h1>ECCV 2024 2nd Workshop on Vision-Centric Autonomous Driving (VCAD)</h1>
        
        <div class="intro" id="intro">
            <h2>Introduction</h2>
            <p>
                With the commercialization of autonomous driving and assisted driving systems, the demand for high-performance, efficient, and scalable machine learning solutions is becoming more urgent than ever before. Visual perception is a key research area of self-driving that is always attracting a lot of attention since (1) visual data provides much richer information than other sensors; (2) cameras are affordable and pervasive on vehicles as well as other robotic systems; (3) visual foundation models are trending directions in machine learning. This workshop embraces topics around vision-centric and data-driven autonomous driving, including but not limited to the following topics:
            </p>
            <ul>
                <li>Visual perception for autonomous driving</li>
                <li>Vision-language models for autonomous driving</li>
                <li>Neural rendering of driving scenes</li>
                <li>Visual world models for autonomous driving</li>
                <li>Vision-centric end-to-end driving</li>
                <li>Visual representation learning for autonomous driving</li>
                <li>Multi-sensory fusion</li>
                <li>Few-shot/semi-supervised/self-supervised learning</li>
                <li>Motion prediction, planning, and simulation</li>
                <li>New datasets and metrics for autonomous driving</li>
                <li>Privacy concerns on visual data</li>
            </ul>
        </div>

        <div class="dates" id="dates">
            <h2>Key Dates</h2>
            <p>
            After the successful holding of <a href="https://vcad.site/#/">CVPR 2023 1st VCAD Workshop</a>, ECCV 2024 2nd VCAD Workshop will take place on <strong>Sept 30, 2024</strong> from <strong>8:00am</strong> to <strong>1:00pm</strong> at <a href="https://www.micomilano.it/en">MiCo Milano</a>.
            </p>
            <ul>
                <li>Submission deadline: <strong>August 15th, 2024 (11:59 pm AOE)</strong></li>
                <li>Acceptance notification: <strong>Sept 9, 2024</strong></li>
                <li>Camera ready for accepted submissions: <strong>Sept 15, 2024</strong></li>
            </ul>
            <p>Please submit your draft to vision-centric-autonomous-driving@outlook.com. Submissions must follow the ECCV 2024 template and will be peer-reviewed in a single-blind manner. Submission must be no more than 14 pages (excluding references). Accepted papers will be presented in the form of posters, with several papers being selected for oral presentations. </p>
        </div>

        <div class="keynotes" id="keynotes">
            <h2>Keynote Speakers</h2>
            <div class="speakers">
                <div class="person">
                    <a href="https://research.nvidia.com/person/jose-m-alvarez">
                        <img src="assets/img/people/jose.jpeg" alt="Jose M. Alvarez">
                        <p>Jose M. Alvarez<br>NVIDIA</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://sergiocasas.github.io/">
                        <img src="assets/img/people/sergio.jpg" alt="Sergio Casas">
                        <p>Sergio Casas<br>Waabi</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://sites.google.com/site/boyilics">
                        <img src="assets/img/people/boyi.jpeg" alt="Boyi Li">
                        <p>Boyi Li<br>UC Berkeley</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://www.linkedin.com/in/fergal-cotter/?originalSubdomain=uk">
                        <img src="assets/img/people/fergal.jpg" alt="Fergal Cotter">
                        <p>Fergal Cotter<br>Wayve</p>
                    </a>
                </div>
                
                
            </div>

            <div class="schedule">
                <h3>Keynote Schedule</h3>
                <p><strong>8:55 am – 9:00 am:</strong> Workshop Introduction</p>
                <p><strong>9:00 am – 9:45 am:</strong> Jose M. Alvarez - Talk: Towards robust and reliable AV with Foundational Models</p>
<!--                 <p><em>Abstract:</em> </p> -->
                <p><strong>9:45 am – 10:30 am:</strong> Sergio Casas - Talk: Perceiving and Forecasting Anything with Self-Supervision</p>
                <p><em>Abstract:</em> Perceiving and forecasting the world are critical tasks for self-driving. Supervised approaches leverage annotated object labels to learn a model of the world -- traditionally with object detections and trajectory predictions, or semantic bird's-eye-view occupancy fields. However, these annotations are expensive and typically limited to a set of predefined categories that do not cover everything we might encounter on the road. Instead, we explore methods to learn to perceive and forecast with self-supervision from LiDAR data. We show that such self-supervised world models can be easily and effectively transferred to downstream tasks.</p>
                <p><strong>10:30 am – 11:00 am:</strong> Poster Sessions</p>
                <p><strong>11:00 am – 11:45 am:</strong> Boyi Li - Talk: Tokenize the World into Object-level Knowledge to Address Long-tail Events in Autonomous Driving</p>
                <p><em>Abstract:</em> The autonomous driving industry is increasingly adopting end-to-end learning from sensory inputs to minimize human biases in system design. Traditional end-to-end driving models, however, suffer from long-tail events due to rare or unseen inputs within their training distributions. To address this, we propose TOKEN, a novel Multi-Modal Large Language Model (MM-LLM) that tokenizes the world into object-level knowledge, enabling better utilization of LLM's reasoning capabilities to enhance autonomous vehicle planning in long-tail scenarios. TOKEN effectively alleviates data scarcity and inefficient tokenization by leveraging a traditional end-to-end driving model to produce condensed and semantically enriched representations of the scene, which are optimized for LLM planning compatibility through deliberate representation and reasoning alignment training stages. Our results demonstrate that TOKEN excels in grounding, reasoning, and planning capabilities, outperforming existing frameworks with a 27% reduction in trajectory L2 error and a 39% decrease in collision rates in long-tail scenarios.</p>
                <p><strong>11:45 am – 12:30 pm:</strong> Fergal Cotter - Talk: Building Digital Cities</p>
                <p><em>Abstract:</em> We explore the space of neural reconstruction methods for resimulation and its importance for self driving. At Wayve, we challenge the status quo of self-driving and advocate heavily for an end-to-end “AV2.0” approach, where the driving model connects directly to sensor inputs and control outputs. In the same vein, we advocate for doing resimulation in the wild with the barest of requirements. In this talk, we look at how we’ve put this into practice at Wayve in building our Ghost-Gym resimulation engine on top of PRISM-1.</p>
             
                <p><strong>12:30 pm – 1:30 pm:</strong> Oral Sessions</p>
            </div>

        </div>

        <div class="organizers" id="organizers">
            <h2>Organizers</h2>
            <div class="speakers">
                <div class="person">
                    <a href="https://yimingli-page.github.io/">
                        <img src="assets/img/people/yiming.jpeg" alt="Yiming Li">
                        <p>Yiming Li<br>NYU & NVIDIA Research</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://yuewang.xyz">
                        <img src="assets/img/people/yue.png" alt="Yue Wang">
                        <p>Yue Wang<br>USC & NVIDIA Research</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://hangzhaomit.github.io">
                        <img src="assets/img/people/hang.jpeg" alt="Hang Zhao">
                        <p>Hang Zhao<br>Tsinghua University</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://scholar.google.com/citations?hl=en&user=ow3r9ogAAAAJ">
                        <img src="assets/img/people/vitor.jpg" alt="Vitor Guizilini">
                        <p>Vitor Guizilini<br>Toyota Research Institute</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://sites.google.com/site/boyilics/home">
                        <img src="assets/img/people/yiyi.jpeg" alt="Yiyi Liao">
                        <p>Yiyi Liao<br>Zhejiang University</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://www.xinshuoweng.com">
                        <img src="assets/img/people/xinshuo.jpeg" alt="Xinshuo Weng">
                        <p>Xinshuo Weng<br>NVIDIA Research</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://web.stanford.edu/~pavone/index.html">
                        <img src="assets/img/people/marco.jpeg" alt="Marco Pavone">
                        <p>Marco Pavone<br>Stanford University & NVIDIA Research</p>
                    </a>
                </div>
            </div>
        </div>

        <div class="contact" id="contact">
            <h2>Contact</h2>
            <p>For any questions, please contact us at <a href="mailto:yimingli@nyu.edu">yimingli@nyu.edu</a>.</p>
        </div>
    </div>

    <footer>
        <p>&copy; 2024 ECCV Workshop on Vision-Centric Autonomous Driving. All rights reserved.</p>
    </footer>
</body>
</html>
