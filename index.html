<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ECCV 2024 2nd Workshop on Vision-Centric Autonomous Driving (VCAD)</title>
    <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Montserrat', sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
            background-color: #f0f2f5;
            color: #333;
        }
        .container {
            width: 90%;
            max-width: 900px;
            margin: 20px auto;
            padding: 20px;
            background: #fff;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
            border-radius: 8px;
        }
        h1, h2, h3, h4, h5 {
            text-align: center;
            color: #1a237e;
        }
        .intro, .dates, .keynotes, .organizers, .contact {
            margin: 20px 0;
        }
        .intro ul, .dates ul {
            list-style: none;
            padding: 0;
        }
        .intro li, .dates li {
            margin: 10px 0;
            padding-left: 20px;
            position: relative;
        }
        .intro li::before, .dates li::before {
            content: '•';
            position: absolute;
            left: 0;
            color: #1a237e;
            font-weight: bold;
        }
        .speakers, .organizers {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
        }
        .person {
            text-align: center;
            margin: 20px;
            width: calc(100% - 40px);
            max-width: 200px;
        }
        .person img {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            object-fit: cover;
            border: 3px solid #1a237e;
            transition: transform 0.3s ease;
        }
        .person img:hover {
            transform: scale(1.1);
        }
        .person p {
            margin: 10px 0 0 0;
        }
        .person a {
            text-decoration: none;
            color: #333;
            font-weight: bold;
        }
        .person a:hover {
            color: #1a237e;
        }
        .contact a {
            color: #1a237e;
            text-decoration: none;
        }
        .contact a:hover {
            text-decoration: underline;
        }
        footer {
            text-align: center;
            margin: 20px 0;
            font-size: 0.9em;
            color: #777;
        }
        nav {
            background-color: #1a237e;
            padding: 10px 0;
            text-align: center;
        }
        nav a {
            color: #fff;
            margin: 0 15px;
            text-decoration: none;
            font-weight: bold;
        }
        nav a:hover {
            text-decoration: underline;
        }
        .submission-button {
            display: block;
            margin: 20px auto;
            padding: 10px 20px;
            background-color: #1a237e;
            color: #fff;
            text-align: center;
            border-radius: 5px;
            text-decoration: none;
            font-weight: bold;
        }
        .submission-button:hover {
            background-color: #303f9f;
        }
        .schedule {
            margin: 20px 0;
        }
        .schedule h3 {
            color: #1a237e;
            margin-bottom: 10px;
        }
        .schedule p {
            margin-bottom: 10px;
        }
        @media (min-width: 600px) {
            .person {
                flex: 1 1 calc(25% - 40px);
                max-width: calc(25% - 40px);
            }
        }
    </style>
</head>
<body>
    <nav>
        <a href="#intro">Introduction</a>
        <a href="#dates">Key Dates</a>
        <a href="#keynotes">Keynote Speakers</a>
        <a href="#organizers">Organizers</a>
        <a href="#contact">Contact</a>
    </nav>

    <div class="container">
        <h1>ECCV 2024 2nd Workshop on Vision-Centric Autonomous Driving (VCAD)</h1>
        
        <div class="intro" id="intro">
            <h2>Introduction</h2>
            <p>
                With the commercialization of autonomous driving and assisted driving systems, the demand for high-performance, efficient, and scalable machine learning solutions is becoming more urgent than ever before. Visual perception is a key research area of self-driving that is always attracting a lot of attention since (1) visual data provides much richer information than other sensors; (2) cameras are affordable and pervasive on vehicles as well as other robotic systems; (3) visual foundation models are trending directions in machine learning. This workshop embraces topics around vision-centric and data-driven autonomous driving, including but not limited to the following topics:
            </p>
            <ul>
                <li>Visual perception for autonomous driving</li>
                <li>Vision-language models for autonomous driving</li>
                <li>Neural rendering of driving scenes</li>
                <li>Visual world models for autonomous driving</li>
                <li>Vision-centric end-to-end driving</li>
                <li>Visual representation learning for autonomous driving</li>
                <li>Multi-sensory fusion</li>
                <li>Few-shot/semi-supervised/self-supervised learning</li>
                <li>Motion prediction, planning, and simulation</li>
                <li>New datasets and metrics for autonomous driving</li>
                <li>Privacy concerns on visual data</li>
            </ul>
        </div>

        <div class="dates" id="dates">
            <h2>Key Dates</h2>
            <p>
            After the successful holding of <a href="https://vcad.site/#/">CVPR 2023 1st VCAD Workshop</a>, ECCV 2024 2nd VCAD Workshop will take place on <strong>Sept 30, 2024</strong> from <strong>8:55am</strong> to <strong>1:30pm</strong> at <a href="https://www.micomilano.it/en">MiCo Milano, Suite 9</a>.
            </p>
            <ul>
                <li>Submission deadline: <strong>August 15th, 2024 (11:59 pm AOE)</strong></li>
                <li>Acceptance notification: <strong>Sept 9, 2024</strong></li>
                <li>Camera ready for accepted submissions: <strong>Sept 15, 2024</strong></li>
            </ul>
            <p>Please submit your draft to vision-centric-autonomous-driving@outlook.com. Submissions must follow the ECCV 2024 template and will be peer-reviewed in a single-blind manner. Submission must be no more than 14 pages (excluding references). Accepted papers will be presented in the form of posters, with several papers being selected for oral presentations. </p>
        </div>

        <div class="keynotes" id="keynotes">
            <h2>Keynote Speakers</h2>
            <div class="speakers">
                <div class="person">
                    <a href="https://research.nvidia.com/person/jose-m-alvarez">
                        <img src="assets/img/people/jose.jpeg" alt="Jose M. Alvarez">
                        <p>Jose M. Alvarez<br>NVIDIA</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://sergiocasas.github.io/">
                        <img src="assets/img/people/sergio.jpg" alt="Sergio Casas">
                        <p>Sergio Casas<br>Waabi</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://sites.google.com/site/boyilics">
                        <img src="assets/img/people/boyi.jpeg" alt="Boyi Li">
                        <p>Boyi Li<br>UC Berkeley</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://www.linkedin.com/in/fergal-cotter/?originalSubdomain=uk">
                        <img src="assets/img/people/fergal.jpg" alt="Fergal Cotter">
                        <p>Fergal Cotter<br>Wayve</p>
                    </a>
                </div>
                
                
            </div>

            <div class="schedule">
    <h3>Keynote Schedule</h3>
    <p><strong>8:55 am – 9:00 am:</strong> Workshop Introduction</p>
    <p><strong>9:00 am – 9:45 am: Jose M. Alvarez - Towards Robust and Reliable AV with Foundational Models</strong></p>
    <p><em>Bio:</em> <a href="https://alvarezlopezjosem.github.io">Dr. Jose M. Alvarez</a> is a research director at NVIDIA, leading the Autonomous Vehicle Applied Research team. His team maximizes the impact of the latest research advances on the AV product. Jose's research interests include model-centric and data-centric deep learning toward more efficient and scalable systems. Jose completed his Ph.D. in computer science in Barcelona, specializing in road-scene understanding for autonomous driving when datasets were very limited. He also worked as a postdoctoral researcher at NYU under Yann LeCunn.</p>
    <p><strong>9:45 am – 10:30 am: Sergio Casas - Perceiving and Forecasting Anything with Self-Supervision</strong></p>
    <p><em>Abstract:</em> Perceiving and forecasting the world are critical tasks for self-driving. Supervised approaches leverage annotated object labels to learn a model of the world -- traditionally with object detections and trajectory predictions, or semantic bird's-eye-view occupancy fields. However, these annotations are expensive and typically limited to a set of predefined categories that do not cover everything we might encounter on the road. Instead, we explore methods to learn to perceive and forecast with self-supervision from LiDAR data. We show that such self-supervised world models can be easily and effectively transferred to downstream tasks.</p>
    <p><em>Bio:</em> <a href="https://sergiocasas.github.io/">Dr. Sergio Casas</a> is a Senior Staff Tech Lead Manager at Waabi, where he leads the Perception and Behavior Reasoning team. He completed his Ph.D. at the University of Toronto, under the supervision of Prof. Raquel Urtasun. His research lies at the intersection of computer vision, machine learning, and robotics.</p>
    <p><strong>10:30 am – 11:00 am:</strong> Poster Sessions</p>
    <ul>
        <li>Foundation Models for Amodal Video Instance Segmentation in Automated Driving</li>
        <li>A New Dataset for Monocular Depth Estimation Under Viewpoint Shifts</li>
        <li>Accuracy Evaluation and Improvement of the Calibration of Stereo Vision Datasets</li>
        <li>Robust Unsupervised Optical Flow Under Low-Visibility Conditions</li>
        <li>RoDUS: Robust Decomposition of Static and Dynamic Elements in Urban Scenes</li>
        <li>FSMDet: Vision-guided feature diffusion for fully sparse 3D detector</li>
        <li>Beyond Entropy: Style Transfer Guided Single Image Continual Test-Time Adaptation</li>
        <li>MapNeXt: Revisiting Training and Scaling Practices for Vectorized HD Map Construction</li>
        <li>Cross-Spectral Gated-RGB Stereo Depth Estimation</li>
        <li>High-Order Evolving Graphs for Enhanced Representation of Traffic Dynamics</li>
        <li>Robust Bird’s Eye View Segmentation by Adapting DINOv2</li>
    </ul>
    
    <p><strong>11:00 am – 11:45 am: Boyi Li - Tokenize the World into Object-level Knowledge to Address Long-tail Events in Autonomous Driving</strong></p>
    <p><em>Abstract:</em> The autonomous driving industry is increasingly adopting end-to-end learning from sensory inputs to minimize human biases in system design. Traditional end-to-end driving models, however, suffer from long-tail events due to rare or unseen inputs within their training distributions. To address this, we propose TOKEN, a novel Multi-Modal Large Language Model (MM-LLM) that tokenizes the world into object-level knowledge, enabling better utilization of LLM's reasoning capabilities to enhance autonomous vehicle planning in long-tail scenarios. TOKEN effectively alleviates data scarcity and inefficient tokenization by leveraging a traditional end-to-end driving model to produce condensed and semantically enriched representations of the scene, which are optimized for LLM planning compatibility through deliberate representation and reasoning alignment training stages. Our results demonstrate that TOKEN excels in grounding, reasoning, and planning capabilities, outperforming existing frameworks with a 27% reduction in trajectory L2 error and a 39% decrease in collision rates in long-tail scenarios.</p>
    <p><em>Bio:</em> <a href="https://sites.google.com/site/boyilics">Dr. Boyi Li</a> is a Research Scientist at NVIDIA Research and a Postdoctoral Scholar at UC Berkeley, where she is advised by Prof. Jitendra Malik and Prof. Trevor Darrell. She received her Ph.D. from Cornell University, under the guidance of Prof. Serge Belongie and Prof. Kilian Q. Weinberger. Dr. Li’s research focuses on learning from multimodal data, developing generalizable algorithms and interactive intelligent systems, and concentrating on areas such as reasoning, large language models, generative models, and robotics. Her work involves aligning representations from multimodal data, including 2D pixels, 3D geometry, language, and audio. </p>
    <p><strong>11:45 am – 12:30 pm: Fergal Cotter - Building Digital Cities</strong></p>
    <p><em>Abstract:</em> We explore the space of neural reconstruction methods for resimulation and its importance for self-driving. At Wayve, we challenge the status quo of self-driving and advocate heavily for an end-to-end “AV2.0” approach, where the driving model connects directly to sensor inputs and control outputs. In the same vein, we advocate for doing resimulation in the wild with the barest of requirements. In this talk, we look at how we’ve put this into practice at Wayve in building our Ghost-Gym resimulation engine on top of PRISM-1.</p>
    <p><em>Bio:</em> <a href="https://fergalcotter.com/">Dr. Fergal Cotter</a> is a Staff Applied Scientist at Wayve, where he has mostly looked at the Perception side of End-to-End driving. In particular, he has focused on how perception and geometry information can be distilled into driving models, as well as ensuring a good grasp of what is happening in the training data. Recently, he has explored the resimulation problem and how driving models can be benchmarked without ever taking them on the road. Before joining Wayve, Dr. Cotter completed his PhD at the University of Cambridge, where he worked on using the wavelet and frequency domain as a good representation and learning space for image understanding.</p>
    <p><strong>12:30 pm – 1:30 pm:</strong> Oral Sessions</p>
    <ul>
        <li><strong>12:30 pm – 12:35 pm:</strong> VL4AD: Vision-Language Models Improve Pixel-wise Anomaly Detection</li>
        <li><strong>12:35 pm – 12:40 pm:</strong> Explanation for Trajectory Planning using Multi-modal Large Language Model for Autonomous Driving</li>
        <li><strong>12:40 pm – 12:45 pm:</strong> ExelMap: Explainable Element-based HD-Map Change Detection and Update</li>
        <li><strong>12:45 pm – 12:50 pm:</strong> Long-Tailed 3D Detection via 2D Late Fusion</li>
        <li><strong>12:50 pm – 12:55 pm:</strong> UL-VIO: Ultra-lightweight Visual-Inertial Odometry with Noise Robust Test-time Adaptation</li>
        <li><strong>12:55 pm – 1:00 pm:</strong> TopoMaskV2: Enhanced Instance-Mask-Based Formulation for the Road Topology Problem</li>
        <li><strong>1:00 pm – 1:05 pm:</strong> Causal Transformer for Fusion and Pose Estimation in Deep Visual Inertial Odometry</li>
        <li><strong>1:05 pm – 1:10 pm:</strong> What Matters to Enhance Traffic Rule Compliance of Imitation Learning for End-to-End Autonomous Driving</li>
        <li><strong>1:10 pm – 1:15 pm:</strong> S3Track: Self-supervised Tracking with Soft Assignment Flow</li>
        <li><strong>1:15 pm – 1:20 pm:</strong> CraftedVoxels: Improving 3D detector accuracy at the Starting Line</li>
        <li><strong>1:20 pm – 1:25 pm:</strong> Accurate 3D Automatic Annotation of Traffic Lights and Signs for Autonomous Driving</li>
    </ul>
</div>


        </div>

        <div class="organizers" id="organizers">
            <h2>Organizers</h2>
            <div class="speakers">
                <div class="person">
                    <a href="https://yimingli-page.github.io/">
                        <img src="assets/img/people/yiming.jpeg" alt="Yiming Li">
                        <p>Yiming Li<br>NYU & NVIDIA Research</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://yuewang.xyz">
                        <img src="assets/img/people/yue.png" alt="Yue Wang">
                        <p>Yue Wang<br>USC & NVIDIA Research</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://hangzhaomit.github.io">
                        <img src="assets/img/people/hang.jpeg" alt="Hang Zhao">
                        <p>Hang Zhao<br>Tsinghua University</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://scholar.google.com/citations?hl=en&user=ow3r9ogAAAAJ">
                        <img src="assets/img/people/vitor.jpg" alt="Vitor Guizilini">
                        <p>Vitor Guizilini<br>Toyota Research Institute</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://sites.google.com/site/boyilics/home">
                        <img src="assets/img/people/yiyi.jpeg" alt="Yiyi Liao">
                        <p>Yiyi Liao<br>Zhejiang University</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://www.xinshuoweng.com">
                        <img src="assets/img/people/xinshuo.jpeg" alt="Xinshuo Weng">
                        <p>Xinshuo Weng<br>NVIDIA Research</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://ai4ce.github.io/">
                        <img src="assets/img/people/chen.jpeg" alt="Chen Feng">
                        <p>Chen Feng<br>New York University</p>
                    </a>
                </div>
                <div class="person">
                    <a href="https://web.stanford.edu/~pavone/index.html">
                        <img src="assets/img/people/marco.jpeg" alt="Marco Pavone">
                        <p>Marco Pavone<br>Stanford University & NVIDIA Research</p>
                    </a>
                </div>
            </div>
        </div>

        <div class="contact" id="contact">
            <h2>Contact</h2>
            <p>For any questions, please contact us at <a href="mailto:yimingli@nyu.edu">yimingli@nyu.edu</a>.</p>
        </div>
    </div>

    <footer>
        <p>&copy; 2024 ECCV Workshop on Vision-Centric Autonomous Driving. All rights reserved.</p>
    </footer>
</body>
</html>
